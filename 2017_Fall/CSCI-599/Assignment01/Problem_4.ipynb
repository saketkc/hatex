{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Visualizations and Class Activation Mapping (CAM)\n",
    "\n",
    "* <b>Learning Objective:</b> In problem 2, and 3, you are asked to use TensorFlow to design and train convolutiional neural networks. In this part of assignment, you are going to play with some fun visualization tricks. We will provide the pretrained models of two well-known CNN architectures: AlexNet and VGG-16 Net to demonstrate visual patterns learned in the filters. In the last part, we introduce Grad-CAM, which is a recent work to visualize the attended heat map where the CNN look at while predicting the labels of the images.\n",
    "* <b>Provided Codes:</b> We provide the necessary code blocks for each section.\n",
    "* <b>TODOs:</b> Follow the instructions to complete the TODO parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A bit of setups\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import gen_nn_ops\n",
    "from lib.datasets import *\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Model\n",
    "Download the vgg16 pretrained model from the link:\n",
    "ftp://mi.eng.cam.ac.uk/pub/mttt2/models/vgg16.npy <br\\ >\n",
    "Download the AlexNet pretrained model from the link [BVLC_ALEXNET](http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/bvlc_alexnet.npy) <br\\ >\n",
    "And save the two models (.npy files) to lib/tf_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.tf_models import vgg16\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing learned filters\n",
    "In this section we will show you how to visualize the learned convolutional filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the vgg network for visualizations\n",
    "vgg_viz = vgg16.Vgg16()\n",
    "vgg_viz.load()\n",
    "vgg_viz.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful function to arrange the images to be shown as a squared grid\n",
    "def viz_grid(Xs, ubound=255.0, padding=1):\n",
    "    N, H, W, C = Xs.shape\n",
    "    grid_size = int(math.ceil(math.sqrt(N)))\n",
    "    grid_height = H * grid_size + padding * (grid_size - 1)\n",
    "    grid_width = W * grid_size + padding * (grid_size - 1)\n",
    "    grid = np.zeros((grid_height, grid_width, C))\n",
    "    next_idx = 0\n",
    "    y0, y1 = 0, H\n",
    "    for y in xrange(grid_size):\n",
    "        x0, x1 = 0, W\n",
    "        for x in xrange(grid_size):\n",
    "            if next_idx < N:\n",
    "                img = Xs[next_idx]\n",
    "                grid[y0:y1, x0:x1] = img\n",
    "                next_idx += 1\n",
    "            x0 += W + padding\n",
    "            x1 += W + padding\n",
    "        y0 += H + padding\n",
    "        y1 += H + padding\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_paths = {\n",
    "    \"Vgg-16\": os.path.join(\"lib/tf_models\", \"vgg16.npy\"),\n",
    "    \"AlexNet\": os.path.join(\"lib/tf_models\", \"bvlc_alexnet.npy\")\n",
    "}\n",
    "\n",
    "for net in sorted(model_paths):\n",
    "    model_path = model_paths[net]\n",
    "    print(\"Model from {}\".format(model_path))\n",
    "    pretrained = np.load(model_path, encoding='latin1').item()\n",
    "    print(\"Pretrained {} successfully loaded!\".format(net))\n",
    "\n",
    "    first_conv = \"conv1_1\" if net == \"Vgg-16\" else \"conv1\"\n",
    "    \n",
    "    conv1 = pretrained[first_conv]\n",
    "    #############################################################################\n",
    "    # TODO: Extract the weight and bias from conv1                              #\n",
    "    # HINT: What's the data type of conv1?                                      #\n",
    "    #############################################################################\n",
    "    \n",
    "    \n",
    "    #############################################################################\n",
    "    # TODO: Scale the kernel weights to [0,1]                                   #\n",
    "    #############################################################################\n",
    "    w_01 = None\n",
    "    \n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Transpose the scaled kernel weights so that the                     #\n",
    "    # number of filters comes first in the dimension as (n, H, W, C)            #\n",
    "    #############################################################################\n",
    "    wT = None\n",
    "\n",
    "    # Define a figure\n",
    "    fig = plt.figure(figsize=(8,8))   \n",
    "    ax1 = plt.subplot(111)\n",
    "\n",
    "    rgb_w = []  # list of filters\n",
    "    n, H, W, C = wT.shape\n",
    "    \n",
    "    #############################################################################\n",
    "    # TODO: Store each of the n transposed kernel weights                       #\n",
    "    # to the rgb_w list so the list is of dimension (n, (H, W, C))              #\n",
    "    #############################################################################\n",
    "    \n",
    "        \n",
    "    # Transform the python list to numpy array\n",
    "    rgb_w = np.asarray(rgb_w)\n",
    "    \n",
    "    # Grid the rgb_w\n",
    "    grid = viz_grid(rgb_w)\n",
    "\n",
    "    ax1.imshow(grid[...,::-1])\n",
    "    ax1.set_title('{} Learned First Conv Filters'.format(net), fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CIFAR-10 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train, labels_train, data_test, labels_test = CIFAR10('data/cifar-10-batches-py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"Scarlet\">Run the following lines of code whenever you encounter problems with tf graph<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers in Problem 2 (Define your layers)\n",
    "Copy and paste your implementation for layers in problem 2 to below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define conv layer and fully-connected layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample convolutional nueral network in Problem 2 (Your own)\n",
    "Copy and paste your implementation for the BaseModel in problem 2 to below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaseModel(object):\n",
    "    def __init__(self):\n",
    "        self.num_epoch = 5\n",
    "        self.batch_size = 128\n",
    "        self.log_step = 50\n",
    "        self._build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the learned filters on your trained CIFAR-10 network!\n",
    "Repeat the above procedure on your own trained network, for the first convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = BaseModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Restore the model using parameters dict\n",
    "variables = tf.global_variables()\n",
    "param_dict = {}\n",
    "for var in variables:\n",
    "    var_name = var.name[:-2]\n",
    "    print('Loading {} from checkpoint. Name: {}'.format(var.name, var_name))\n",
    "    param_dict[var_name] = var\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"lib/tf_models/problem2/csci-599_sample.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "with sess.as_default():\n",
    "    with tf.variable_scope(\"conv1\", reuse=True):\n",
    "        conv1 = tf.get_variable(\"w\")\n",
    "        b1    = tf.get_variable(\"b\")\n",
    "    print conv1.shape, b1.shape\n",
    "    \n",
    "    #############################################################################\n",
    "    # TODO: Extract the weight and bias from conv1                              #\n",
    "    # For tf models, you should use .eval() function on variables and           #\n",
    "    # sess.run() function to extract the features to ndarray                    #\n",
    "    #############################################################################\n",
    "    \n",
    "    \n",
    "    #############################################################################\n",
    "    # TODO: Scale the kernel weights to [0,1]                                   #\n",
    "    #############################################################################\n",
    "    w_01 = None\n",
    "    \n",
    "    #############################################################################\n",
    "    # TODO: Transpose the scaled kernel weights so that the                     # \n",
    "    # number of filters comes first in the dimension (n, H, W, C)               #\n",
    "    #############################################################################\n",
    "    wT = \n",
    "\n",
    "    # Define a figure\n",
    "    fig = plt.figure(figsize=(8,8))   \n",
    "    ax1 = plt.subplot(111)\n",
    "\n",
    "    rgb_w = []  # list of filters\n",
    "    n, H, W, C = wT.shape\n",
    "    \n",
    "    #############################################################################\n",
    "    # TODO: Store each of the n transposed kernel weights                       #\n",
    "    # to the rgb_w list so the list is of dimension (n, (H, W, C))              #\n",
    "    #############################################################################\n",
    "    \n",
    "    \n",
    "    grid = viz_grid(rgb_w)\n",
    "\n",
    "    ax1.imshow(grid[...,::-1])\n",
    "    ax1.set_title('{} Learned First Conv Filters'.format(net), fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Activations\n",
    "Now we saw the learned filters, and observed that they have some patterns. We will go one step forward to visualize the activation maps produced by different convolutional filters. You will see that as we go deeper through the layers of a network the activation maps gradually represent higher and higher levels of abstraction in the images. <br\\ >\n",
    "Now, let's get some warm-ups by running the following visualization code blocks for a simple model trained on MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784],name=\"x-in\")\n",
    "y = tf.placeholder(tf.float32, [None, 10],name=\"y-in\")\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "x_reshaped = tf.reshape(x,[-1,28,28,1])\n",
    "x_tiled = tf.tile(x_reshaped, [1,1,1,3])\n",
    "sconv_1 = slim.conv2d(x_tiled,5,[5,5])\n",
    "spool_1 = slim.max_pool2d(sconv_1,[2,2])\n",
    "sconv_2 = slim.conv2d(spool_1,5,[5,5])\n",
    "spool_2 = slim.max_pool2d(sconv_2,[2,2])\n",
    "sconv_3 = slim.conv2d(spool_2,20,[5,5])\n",
    "s_dropout3 = slim.dropout(sconv_3, keep_prob)\n",
    "output = slim.fully_connected(slim.flatten(s_dropout3), 10, activation_fn=tf.nn.softmax)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y * tf.log(output))\n",
    "correct_prediction = tf.equal(tf.argmax(output,1 ), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batchSize = 50\n",
    "dropout_p = 0.5\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Train the network\n",
    "for i in range(2001):\n",
    "    batch = mnist.train.next_batch(batchSize)\n",
    "    sess.run(train_step, feed_dict={x:batch[0], y:batch[1], keep_prob:dropout_p})\n",
    "    if i % 100 == 0 and i != 0:\n",
    "        trainAccuracy = sess.run(accuracy, feed_dict={x:batch[0], y:batch[1], keep_prob:1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, trainAccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testAccuracy = sess.run(accuracy, feed_dict={x:mnist.test.images,y:mnist.test.labels, keep_prob:1.0})\n",
    "print(\"test accuracy {}\".format(testAccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for visualizing the activations\n",
    "def getActivations_mnist(layer, features):\n",
    "    outs = sess.run(layer, feed_dict={x:np.reshape(features,[1,784],order='F'), keep_prob:1.0})\n",
    "    outs = np.transpose(outs, [3, 1, 2, 0])\n",
    "    fig = plt.figure(figsize=(4,4))   \n",
    "    ax1 = plt.subplot(111)\n",
    "    grid = viz_grid(outs)\n",
    "    ax1.imshow(grid[...,0])\n",
    "    ax1.set_title('{} Activations'.format(layer.name), fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imageToUse = mnist.test.images[0]\n",
    "imageToShow = np.expand_dims(np.reshape(imageToUse,[28,28]), axis=-1)\n",
    "imageToShow = np.tile(imageToShow, (1,1,3))\n",
    "plt.imshow(imageToShow, interpolation=\"nearest\", cmap=\"gray\")\n",
    "print \"The Image for activation visualizations:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the first 3 activation maps after convs\n",
    "getActivations_mnist(sconv_1,imageToUse)\n",
    "getActivations_mnist(sconv_2,imageToUse)\n",
    "getActivations_mnist(sconv_3,imageToUse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the activations on your own model\n",
    "Now repeat the above procedure on your own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getActivations_cifar10(layer, stimuli):\n",
    "    #############################################################################\n",
    "    # TODO: Fill out the following block                                        #\n",
    "    #############################################################################\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"There are total {} images in test set\".format(len(data_test))\n",
    "#############################################################################\n",
    "# TODO: Try out some indices you want to see!                               #\n",
    "#############################################################################\n",
    "query_idx = 999\n",
    "\n",
    "# Process the indicated issue\n",
    "query_idx = min(max(query_idx, 0), 999)\n",
    "cifar10ToUse = data_test[query_idx]\n",
    "cifar10ToUse= cifar10ToUse[...,::-1]\n",
    "plt.imshow(cifar10ToUse)\n",
    "print \"Image {} in test set\".format(query_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "# Restore the model using parameters dict\n",
    "model = BaseModel()\n",
    "variables = tf.global_variables()\n",
    "param_dict = {}\n",
    "for var in variables:\n",
    "    var_name = var.name[:-2]\n",
    "    print('Loading {} from checkpoint. Name: {}'.format(var.name, var_name))\n",
    "    param_dict[var_name] = var\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"lib/tf_models/problem2/csci-599_sample.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cifar10ToUse_with_batch = np.expand_dims(cifar10ToUse, axis=0)\n",
    "#############################################################################\n",
    "# TODO: Visualize the activations of each conv layer in your model          #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline Question: Describe your observation of how the activation changes throughout the network below:\n",
    "#### Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-weighted Class Activation Mapping (Grad-CAM)\n",
    "Grad-CAM is a technique for \"visually interpreting\" the predictions of a Convolutional Neural\n",
    "Network (CNN)-based model. This technique essentially uses the gradients of any target concept\n",
    "(a predicted class such as \"cat\"), flowing into the final convolutional layer to produce a coarse localization\n",
    "map attending regions in the image that are important for prediction of the concept.\n",
    "Please read the original paper [Grad-CAM](https://arxiv.org/pdf/1610.02391.pdf) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the guided backpropagation ReLU (Run this function only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace vanila relu to guided relu to get guided backpropagation.\n",
    "@ops.RegisterGradient(\"GuidedRelu\")\n",
    "def _GuidedReluGrad(op, grad):\n",
    "    return tf.where(0. < grad, gen_nn_ops._relu_grad(grad, op.outputs[0]), tf.zeros(grad.get_shape()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def imgread(path):\n",
    "    print \"Image:\", path.split(\"/\")[-1]\n",
    "    # Read in the image using python opencv\n",
    "    img = cv2.imread(path)\n",
    "    img = img / 255.0\n",
    "    print \"Raw Image Shape: \", img.shape\n",
    "    \n",
    "    # Center crop the image\n",
    "    short_edge = min(img.shape[:2])\n",
    "    W, H, C = img.shape\n",
    "    to_crop = min(W, H)\n",
    "    cent_w = int((img.shape[1] - short_edge) / 2)\n",
    "    cent_h = int((img.shape[0] - short_edge) / 2)\n",
    "    img_cropped = img[cent_h:cent_h+to_crop, cent_w:cent_w+to_crop]\n",
    "    print \"Cropped Image Shape: \", img_cropped.shape\n",
    "    \n",
    "    # Resize the cropped image to 224 by 224 for VGG16 network\n",
    "    img_resized = cv2.resize(img_cropped, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "    print \"Resized Image Shape: \", img_resized.shape\n",
    "    return img_resized\n",
    "\n",
    "def predicted_labels(score, synset_path):\n",
    "    fi = open(synset_path, \"rb\")\n",
    "    synset = []\n",
    "    for line in fi:\n",
    "        synset.append(line.rstrip().lstrip())\n",
    "    \n",
    "    # The predictions, reverse ordered\n",
    "    pred = np.argsort(score)[::-1]\n",
    "\n",
    "    # Top 1 and Top 5\n",
    "    top1 = synset[pred[0]]\n",
    "    print \"\\nTop1, Label: {}, score: {}\".format(top1, score[pred[0]])\n",
    "    top5 = [(synset[pred[i]], score[pred[i]]) for i in xrange(5)]\n",
    "    for i in xrange(1,5):\n",
    "        print \"Top{}, Label: {} score: {}\".format(i+1, top5[i][0], top5[i][1])\n",
    "    return top1, top5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize(image, output, grads, gb_grads):\n",
    "    # Reverse the BGR channel to RGB\n",
    "    gb_grads = gb_grads[...,::-1]\n",
    "\n",
    "    # Initialzie CAM weights\n",
    "    CAM = np.ones(output.shape[0 : 2], dtype = np.float32)  \n",
    "\n",
    "    # Taking a weighted average\n",
    "    cam_w = np.mean(grads, axis = (0, 1))\n",
    "    for i, w in enumerate(cam_w):\n",
    "        CAM += w * output[:, :, i]\n",
    "\n",
    "    # Passing through ReLU\n",
    "    CAM = np.maximum(CAM, 0)\n",
    "    # scale CAM to [0,1]\n",
    "    CAM /= np.max(CAM)\n",
    "    # Resize the CAM to 224 by 224\n",
    "    CAM = cv2.resize(CAM, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # scale guided backprop gradients to [0,1]\n",
    "    gb_grads -= np.min(gb_grads)\n",
    "    gb_grads /= np.max(gb_grads)\n",
    "    \n",
    "    # scale the original to [0,1]\n",
    "    img_toshow = image.astype(float)    \n",
    "    img_toshow -= np.min(img_toshow)\n",
    "    img_toshow /= img_toshow.max()\n",
    "\n",
    "    # Render the CAM heatmap\n",
    "    heatmap = cv2.applyColorMap(np.uint8(CAM*255.0), cv2.COLORMAP_JET)\n",
    "\n",
    "    # Grad-CAM\n",
    "    CAM_gb = CAM.copy()\n",
    "    CAM_gb = np.expand_dims(np.squeeze(CAM_gb), axis=-1)\n",
    "    gd_gb = gb_grads * np.tile(CAM_gb, (1,1,3))\n",
    "    \n",
    "    # Draw the results figures\n",
    "    fig = plt.figure(figsize=(10,10))   \n",
    "    ax1 = plt.subplot(221)\n",
    "    ax2 = plt.subplot(222)\n",
    "    ax3 = plt.subplot(223)\n",
    "    ax4 = plt.subplot(224)\n",
    "    \n",
    "    ax1.imshow(img_toshow[...,::-1])\n",
    "    ax1.set_title('Input Image')\n",
    "    ax2.imshow(heatmap)\n",
    "    ax2.set_title('Grad-CAM')\n",
    "    ax3.imshow(gb_grads)\n",
    "    ax3.set_title('guided backpropagation')\n",
    "    ax4.imshow(gd_gb)\n",
    "    ax4.set_title('guided Grad-CAM')\n",
    "\n",
    "    # Show the resulting image\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad-CAM Main Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_classes = 1000\n",
    "\n",
    "# Read in the image\n",
    "img1 = imgread(\"images/corgi.jpg\")\n",
    "img2 = imgread(\"images/cat_and_dog.jpg\")\n",
    "img3 = imgread(\"images/cat_and_dog.jpg\")\n",
    "\n",
    "# Expand one dimension to take on the batch dimension\n",
    "img1 = np.expand_dims(img1, axis=0)\n",
    "img2 = np.expand_dims(img2, axis=0)\n",
    "img3 = np.expand_dims(img3, axis=0)\n",
    "\n",
    "# Define a all zero gradients of the shape 1000\n",
    "zero_grads = np.array([0 for i in xrange(num_classes)])\n",
    "\n",
    "# The indices of the classes are provided for you\n",
    "class_num1 = 263  # Pembroke, Pembroke Welsh corgi\n",
    "class_num2 = 254  # Pug, pug-dog\n",
    "class_num3 = 282  # Tiger cat\n",
    "\n",
    "# Define a one-hot gradient vector where the only activated gradient\n",
    "# is of the corresponding indices from above \n",
    "one_hot_grad1 = zero_grads.copy()\n",
    "one_hot_grad2 = zero_grads.copy()\n",
    "one_hot_grad3 = zero_grads.copy()\n",
    "one_hot_grad1[class_num1] = 1.0\n",
    "one_hot_grad2[class_num2] = 1.0\n",
    "one_hot_grad3[class_num3] = 1.0\n",
    "one_hot_grad1 = np.expand_dims(one_hot_grad1, axis=0)\n",
    "one_hot_grad2 = np.expand_dims(one_hot_grad2, axis=0)\n",
    "one_hot_grad3 = np.expand_dims(one_hot_grad3, axis=0)\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Construct a minibatch of data and labels (one-hot vectors) of the   #\n",
    "# images using concatenate                                                  #\n",
    "#############################################################################\n",
    "minibatch = None\n",
    "one_hot_grads = None\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 3\n",
    "\n",
    "# Create tensorflow graph for evaluation\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with graph.gradient_override_map({'Relu': 'GuidedRelu'}):\n",
    "        # Define the VGG16 network and setup\n",
    "        # Please take a look at the lib/tf_models/vgg16.py for more details\n",
    "        # of the VGG16 network\n",
    "        vgg = vgg16.Vgg16()\n",
    "        vgg.load()\n",
    "        vgg.setup()\n",
    "        \n",
    "        #############################################################################\n",
    "        # TODO: Define the signal and the loss                                      #\n",
    "        # HINT: To construnct the signal, simply extract the final fully connected  #\n",
    "        # layer and perform a matrix multiplication on the one-hot vectors          #\n",
    "        # The loss is then defined as the average of the signal                     #\n",
    "        #############################################################################\n",
    "        signal = None\n",
    "        loss = None\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO: Compute the gradient of pool5 layer for generating Grad-CAM         #\n",
    "        #############################################################################\n",
    "        pool5 = None\n",
    "        pool5_grads = None\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO: Perform a guided backpropagtion back to the input layer             #\n",
    "        #############################################################################\n",
    "        gb_grad = None\n",
    "\n",
    "        eps = tf.constant(1e-5)\n",
    "        #############################################################################\n",
    "        # TODO: Normalize the gradients, and add a small number epsilon to it       #\n",
    "        #############################################################################\n",
    "        pool5_grads_norm = None\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO: Initializer for the tf variables                                    #\n",
    "        #############################################################################\n",
    "        init = None\n",
    "\n",
    "# Run tensorflow \n",
    "with tf.Session(graph=graph) as sess:    \n",
    "    sess.run(init)\n",
    "    #############################################################################\n",
    "    # TODO: Run the session to get guided backpropagation gradients to the      #\n",
    "    # input, activation of pool5, normalized pool5 gradients, and the           #\n",
    "    # prediction probability                                                    #\n",
    "    #############################################################################\n",
    "    prob, gb_grads, pool5_act, pool5_grads = None, None, None, None\n",
    "    # Visualize the Grad-CAM\n",
    "    for i in xrange(batch_size):\n",
    "        top1, top5 = predicted_labels(prob[i], \"lib/synset.txt\")\n",
    "        visualize(minibatch[i], pool5_act[i], pool5_grads[i], gb_grads[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Inline Question: Describe your observation of the above results:\n",
    "#### Ans:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
